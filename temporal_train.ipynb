{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e9d3b-f5bc-416e-b794-2be2e4137d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dc479-efad-4fc5-957a-9cac0c0c9579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import glob\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from temporal_dataset import PorosityDataset\n",
    "from temporal_model import *\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e007c-91cb-436d-bc12-dfca58e18db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets:\n",
    "data_dir = '/ix1/xjia/yuw253/porosity/Ksection_porosity_fused_updated_ratio'\n",
    "dataset = PorosityDataset(data_dir, use_padding=False)\n",
    "\n",
    "# Create random splits and compute normalization factors:\n",
    "train_idx, val_idx, test_idx = dataset.get_split(seed=42)\n",
    "\n",
    "dataset_trn = Subset(dataset, train_idx)\n",
    "dataset_val = Subset(dataset, val_idx)\n",
    "dataset_tst = Subset(dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae2688-90b1-42c9-a668-03c3f5e38bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device, task, task_id=-1, verbose=False):\n",
    "    model.train()\n",
    "    if verbose:\n",
    "        pbar = tqdm(dataloader)\n",
    "    else:\n",
    "        pbar = dataloader\n",
    "        \n",
    "    loss_history = []\n",
    "    for batch in pbar:\n",
    "        feats, labels = batch\n",
    "\n",
    "        if task == 'regression':\n",
    "            feats, labels = feats.to(device), labels[:, 1:].to(device)\n",
    "            logits = model(feats)\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1).float())\n",
    "        else:\n",
    "            feats, labels = feats.to(device), labels[:, 0].to(device)\n",
    "            logits = model(feats)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels.view(-1, 1).float())\n",
    "            loss += torch.max(torch.tensor(0).to(device), (torch.sigmoid(model.var_weight / model.temperature)).mean() - 0.6)\n",
    "            \n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_history\n",
    "        \n",
    "\n",
    "def eval_one_epoch(model, dataloader, device, task, task_id=-1, verbose=False):\n",
    "    model.eval()\n",
    "    loss_history = []\n",
    "    references, predictions = [], []\n",
    "    if verbose:\n",
    "        pbar = tqdm(dataloader)\n",
    "    else:\n",
    "        pbar = dataloader\n",
    "        \n",
    "    for batch in pbar:\n",
    "        feats, labels = batch\n",
    "        if task == 'regression':\n",
    "            feats, labels = feats.to(device), labels[:, 1:].to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(feats)\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1).float())\n",
    "            preds = logits\n",
    "        else:\n",
    "            feats, labels = feats.to(device), labels[:, 0].to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(feats)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels.view(-1, 1).float())\n",
    "            preds = torch.sigmoid(logits)\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        references.append(labels.cpu())\n",
    "        predictions.append(preds.squeeze(1).cpu())\n",
    "\n",
    "    references = torch.concat(references)\n",
    "    predictions = torch.concat(predictions)\n",
    "\n",
    "    if task == 'regression':\n",
    "        scores = []\n",
    "        for j in range(references.shape[1]):\n",
    "            scores.append(root_mean_squared_error(predictions[:, j], references[:, j]))\n",
    "            # scores.append(r2_score(predictions[:, j], references[:, j]))\n",
    "    else:\n",
    "        scores = ((predictions > 0.5) == references).float().mean().item()\n",
    "    return loss_history, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e419b-25b1-445f-ad02-8a42d8ca93cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task = 'classification'\n",
    "\n",
    "trn_batch_size = 2048\n",
    "val_batch_size = 1024\n",
    "max_epoch = 50\n",
    "\n",
    "dataloader_trn = DataLoader(dataset_trn, batch_size=trn_batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=val_batch_size, shuffle=False)\n",
    "dataloader_tst = DataLoader(dataset_tst, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "model = LSTM(input_size=7, hidden_size=256, num_layers=5, output_size=1).to(DEVICE)  \n",
    "# model = MLP(in_dim=7, embed_dim=512, out_dim=output_dim, num_layer=5).to(DEVICE)\n",
    "# model = TemporalTransformer(input_size=7, hidden_size=256, num_layers=5, output_size=1).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-03, weight_decay=0.0001)\n",
    "best_acc, best_checkpoint = -float('inf'), None\n",
    "train_loss_list, val_loss_list = [], []\n",
    "for epoch in tqdm(range(max_epoch)):\n",
    "    trn_loss = train_one_epoch(model, dataloader_trn, optimizer, DEVICE, task=task, task_id=task_id, verbose=True)\n",
    "    val_loss, val_acc = eval_one_epoch(model, dataloader_val, DEVICE, task=task, task_id=task_id, verbose=False)\n",
    "    if isinstance(val_acc, list):\n",
    "        val_acc = np.mean(val_acc)\n",
    "    print('[epoch {}] trn_loss={} val_loss={} val_acc={}'.format(\n",
    "        epoch+1, round(np.mean(trn_loss), 3), round(np.mean(val_loss), 3), round(val_acc, 3)))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_checkpoint = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    train_loss_list.append(np.mean(trn_loss))\n",
    "    val_loss_list.append(np.mean(val_loss))\n",
    "\n",
    "# Compute test accuracy\n",
    "model.load_state_dict(best_checkpoint)\n",
    "_, tst_acc = eval_one_epoch(model, dataloader_tst, DEVICE, task=task, task_id=task_id, verbose=False)\n",
    "tst_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d015a-3125-4477-80f3-ec7febaf84b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90b236-6d81-474c-adc5-2f6c86094396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c84ab-aa52-414b-9f88-f8eeda8985d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68f67f-b4fb-4e2a-b0f4-39050b4a6434",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c671af7-ab00-45a3-ad9a-7291ec54c366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd422c0-9ca6-49af-ae87-a685bd39da80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2a988-6faa-48d0-a3d4-4311ac1060e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700e76d-a4ef-446d-82f5-b61c314262eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968dc0af-8539-4f5d-b5e8-37f7fca6f768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc01c28-deee-4ccd-9d3d-05c7d4cdc1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d5927-e99c-4d5c-bc3d-9294e55d1eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2cae4-9a8b-4572-bf7d-36177474e5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
